{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numba\n",
      "  Downloading numba-0.60.0-cp312-cp312-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba)\n",
      "  Downloading llvmlite-0.43.0-cp312-cp312-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting numpy<2.1,>=1.22 (from numba)\n",
      "  Downloading numpy-2.0.2-cp312-cp312-win_amd64.whl.metadata (59 kB)\n",
      "Downloading numba-0.60.0-cp312-cp312-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.7 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.8/2.7 MB 2.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.6/2.7 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.4/2.7 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 3.1 MB/s eta 0:00:00\n",
      "Downloading llvmlite-0.43.0-cp312-cp312-win_amd64.whl (28.1 MB)\n",
      "   ---------------------------------------- 0.0/28.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.6/28.1 MB 7.6 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 3.1/28.1 MB 7.7 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 5.0/28.1 MB 8.1 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 6.6/28.1 MB 8.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 8.4/28.1 MB 8.3 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 11.0/28.1 MB 9.0 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 12.8/28.1 MB 9.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 15.7/28.1 MB 9.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 18.9/28.1 MB 10.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 20.2/28.1 MB 10.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 22.5/28.1 MB 10.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 24.6/28.1 MB 10.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 26.7/28.1 MB 10.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 28.1/28.1 MB 10.0 MB/s eta 0:00:00\n",
      "Downloading numpy-2.0.2-cp312-cp312-win_amd64.whl (15.6 MB)\n",
      "   ---------------------------------------- 0.0/15.6 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 4.2/15.6 MB 20.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 7.6/15.6 MB 19.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.8/15.6 MB 19.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.5/15.6 MB 19.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.6/15.6 MB 18.5 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy, llvmlite, numba\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.2\n",
      "    Uninstalling numpy-2.1.2:\n",
      "      Successfully uninstalled numpy-2.1.2\n",
      "Successfully installed llvmlite-0.43.0 numba-0.60.0 numpy-2.0.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install ollama\n",
    "!pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "import ollama\n",
    "\n",
    "# load all env variables \n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abraham Lincoln issued the Emancipation Proclamation on January 1, 1863, as a war measure to weaken the Confederacy during the American Civil War. The proclamation declared that all slaves in states still in rebellion against the Union were free, effective immediately.\n",
      "\n",
      "There were several reasons why Lincoln issued the Emancipation Proclamation:\n",
      "\n",
      "1. **Military strategy**: Lincoln believed that freeing the slaves would deprive the Confederacy of a significant source of labor and weaken its ability to wage war. By reducing the number of slaves available for work, he hoped to disrupt the Confederate economy and ultimately bring an end to the conflict.\n",
      "2. **International pressure**: The Emancipation Proclamation was also intended to influence international opinion and put pressure on European powers not to recognize the Confederacy as a legitimate nation. Many in Europe saw slavery as morally reprehensible, and by issuing the proclamation, Lincoln hoped to sway public opinion against the Confederacy.\n",
      "3. **Domestic politics**: Lincoln's decision to issue the Emancipation Proclamation was also driven by domestic politics. He wanted to appeal to the abolitionist movement, which had been gaining momentum in the North during the war. By taking a bold stance on slavery, he hoped to win the support of Northern Democrats and other wavering Republicans.\n",
      "4. **Personal conviction**: Lincoln was deeply troubled by the morality of slavery and had long opposed its expansion into new territories. He saw the Emancipation Proclamation as an opportunity to make a significant statement against the injustice of slavery and to promote a more perfect union.\n",
      "\n",
      "It's worth noting that the Emancipation Proclamation did not apply to border states or areas already under Union control, nor did it immediately free all slaves in these regions. However, it paved the way for the eventual abolition of slavery with the passage of the 13th Amendment to the US Constitution in 1865.\n",
      "\n",
      "Overall, Lincoln's decision to issue the Emancipation Proclamation was driven by a combination of military strategy, international pressure, domestic politics, and personal conviction.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=\"llama3.2\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"Why did Lincoln issue the Emancipation Proclamation?\"\"\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1:\n",
    "Let's say we have Q questions, D Datasets, and A answers for each question. We will split P fraction of Q Questions for fine-tuning with golden documents($D_g$: which contains answers to the questions), and the rest (1-P) of Q questions will be trained with distractor documents($D_d$: which are unrelated to the questions). While tuning with distractor documents, model will be forced to learn from the golden documents to answer the 1-P fraction of questions. <b>(Zhang et al.2024)</b>\n",
    "\n",
    "## Approach 2: (LightRAG)\n",
    "Use graph structures for text indexing and relevant information retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=mE7IDf2SmJg&t=1044s\n",
    "\n",
    "## For Document Encodeer\n",
    "# TF-IDF and BM25 \n",
    "# DrQA Chen et.al 2017\n",
    "# OrQA lee et.al 2019\n",
    "# Dens Passage retrieval\n",
    "# Faiss a vector database\n",
    "# ColBert\n",
    "# SPLADE Formal et al. 2021\n",
    "# DRAGON Lin et al.2023\n",
    "\n",
    "## For Retrieval of Docs \n",
    "# RePlug Shi et.al 2023\n",
    "# In-Context RALM\n",
    "\n",
    "## Both retriever and generator\n",
    "# FiD Izacard & Grave 2020\n",
    "# KNN-LM (Khandelwal 2019)\n",
    "# From Document Encoder to Generator\n",
    "# REALM(Gu et al 2020)\n",
    "# Atlas()\n",
    "\n",
    "# Train at scale: TRIME Zhong et al 2023\n",
    "# Self-RAG (Asai et.al 2023)\n",
    "#WebGPT\n",
    "# InstructRetro Wang et.al 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use TriviaQA dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documnet Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we need to do document chunking. There are two ways: For sparse encoding use BM25.\n",
    "#Sparse Retrieval\n",
    "# define BM25\n",
    "import numpy as np\n",
    "import os\n",
    "# find frequeency of words in the documents\n",
    "FOLDER_PATH = 'triviaqa/evidence/web'\n",
    "# get all files under FOLDER_PATH\n",
    "data_files = []\n",
    "for (_,dirs,file_ls) in os.walk(FOLDER_PATH):\n",
    "    for file in file_ls:\n",
    "        data_files.append(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.' 'What' 'word']\n",
      "Extracting finished for:  0_1000687.txt\n",
      "{'1': 3, '10': 1, '1450': 1, '1855': 2, '1928': 1, '1936': 1, '1960': 1, '3': 2, '4': 2, '5': 1, '6': 1, '7': 2, '8': 2, '9': 1, 'answers': 1, 'alfred': 1, 'anne': 1, 'brigade': 2, 'british': 1, 'brontë': 1, 'carnegie': 1, 'charge': 2, 'chatterley': 1, 'christopher': 1, 'cogito': 1, 'conrad': 1, 'could': 1, 'd': 1, 'dale': 1, 'descartes': 1, 'echo': 1, 'email': 1, 'facebook': 1, 'friends': 1, 'get': 1, 'gutenberg': 1, 'h': 1, 'how': 2, 'i': 2, 'in': 1, 'influence': 1, 'jade': 1, 'korzeniowski': 1, 'lady': 1, 'lawrence': 1, 'light': 2, 'like': 1, 'liverpool': 1, 'lord': 1, 'lover': 1, 'marlowe': 1, 'news': 1, 'novella': 1, 'opinion': 1, 'people': 1, 'rené': 1, 'share': 1, 'subscribe': 1, 'tennyson': 1, 'the': 10, 'twice': 1, 'uk': 1, 'what': 1, 'which': 2, 'who': 6, 'win': 1, 'wright': 1, 'a': 5, 'again': 1, 'am': 1, 'and': 3, 'as': 1, 'ban': 1, 'be': 1, 'between': 2, 'blank': 1, 'book': 4, 'by': 1, 'create': 1, 'daily': 1, 'directly': 1, 'dramatist': 1, 'ergo': 1, 'exponent': 1, 'extended': 2, 'fictional': 2, 'first': 1, 'folded': 1, 'four': 1, 'from': 2, 'great': 1, 'inbox': 1, 'invented': 1, 'is': 1, 'laterinvalid': 1, 'leaves': 1, 'lifted': 1, 'many': 1, 'maxim': 1, 'more': 2, 'movable': 1, 'naturalised': 1, 'not': 1, 'novelist': 1, 'of': 6, 'on': 2, 'poem': 2, 'popular': 2, 'printing': 1, 'publishing': 1, 'quarto': 1, 'real': 1, 'refers': 2, 'regarded': 1, 'revolutionising': 1, 'seminal': 1, 'sheet': 1, 'sisters': 1, 'sum': 1, 'surname': 1, 'term': 2, 'therefore': 1, 'think': 1, 'three': 1, 'thus': 1, 'times': 1, 'to': 5, 'try': 1, 'type': 1, 'updates': 1, 'us': 1, 'verse': 1, 'was': 3, 'word': 2, 'words': 2, 'would': 1, 'writing': 1, 'wrote': 4, 'youngest': 1, 'your': 1, 'š': 1, 'šjohannes': 1, 'šjoseph': 1}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk import data as nltk_data\n",
    "\n",
    "nltk_data.path.append(os.environ[\"NLTK_DATA\"])\n",
    "\n",
    "file_path='.\\\\samples\\\\web\\\\0\\\\0_1000687.txt'\n",
    "\n",
    "\n",
    "# code\n",
    "rem_specials = lambda w: ''.join([c for c in w if c.isalnum()])\n",
    "rem_special_words = lambda w: w.isalnum()\n",
    "vec_rem_special_words = np.vectorize(rem_special_words)\n",
    "vec_rem_sp = np.vectorize(rem_specials)\n",
    "file_name = file_path.split(\"\\\\\")[-1]\n",
    "# print(\"Extracting metrics from file: \", file_name)\n",
    "with open(file_path, encoding=\"utf-8\") as file:\n",
    "    # Store words and its count\n",
    "    words={}\n",
    "    # Read file\n",
    "    data = file.read()\n",
    "    # Tokenize entire text into words\n",
    "    word_ls = np.array(word_tokenize(data)) \n",
    "    doclen = len(word_ls)\n",
    "    print(word_ls[1:4])\n",
    "    # Remove all non-alnum characters\n",
    "    word_ls = word_ls[vec_rem_special_words(word_ls)]\n",
    "    # Remove special characters within the words\n",
    "    word_ls = vec_rem_sp(word_ls)\n",
    "    # Count all the unique words in the count\n",
    "    unique_words, counts = np.unique(word_ls, return_counts=True)\n",
    "    # Assign each word and its count in the return var\n",
    "    for index,word in enumerate(unique_words):\n",
    "        word = word.lower()\n",
    "        words[word] = int(counts[index])\n",
    "    print(\"Extracting finished for: \", file_name)\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class s:\n",
    "    def __init__(self):\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
